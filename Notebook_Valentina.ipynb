{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import platform\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from annotations import *\n",
    "from extract_video_features import *\n",
    "from extract_audio_features import *\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.metrics import f1_score\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import ordinal_classification as o_c\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/valentinadiproietto/filrouge'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenFace_folder = '/Users/valentinadiproietto/OpenFace'\n",
    "filename_annotations = 'https://docs.google.com/spreadsheets/d/1Rqu1sJiD-ogc4a6R491JTiaYacptOTqh6DKqhwTa8NA/gviz/tq?tqx=out:csv&sheet=Template'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video_folder = '/Users/valentinadiproietto/Desktop/video_stress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths, video_names = get_videos(Video_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['WIN_20210331_21_22_52_Pro',\n",
       " 'WIN_20210329_14_13_45_Pro',\n",
       " 'WIN_20210406_18_49_10_Pro',\n",
       " 'WIN_20210408_14_11_32_Pro',\n",
       " 'WIN_20210408_15_20_51_Pro',\n",
       " 'WIN_20210404_10_58_27_Pro',\n",
       " 'WIN_20210414_06_24_52_Pro',\n",
       " 'WIN_20210406_15_06_15_Pro',\n",
       " 'WIN_20210417_14_53_12_Pro',\n",
       " 'WIN_20210413_15_38_01_Pro',\n",
       " 'WIN_20210408_11_48_58_Pro',\n",
       " 'WIN_20210408_16_04_32_Pro',\n",
       " 'WIN_20210329_10_16_02_Pro',\n",
       " 'WIN_20210323_19_17_40_Pro',\n",
       " 'WIN_20210409_10_26_11_Pro',\n",
       " 'Test_pour_AFPA',\n",
       " 'WIN_20210405_15_09_16_Pro',\n",
       " 'WIN_20210407_14_54_56_Pro_edit2',\n",
       " 'WIN_20210406_21_05_52_Pro',\n",
       " 'WIN_20210403_18_49_15_Pro',\n",
       " 'WIN_20210408_14_02_19_Pro',\n",
       " 'WIN_20210415_15_41_24_Pro',\n",
       " 'WIN_20210406_18_35_52_Pro',\n",
       " 'WIN_20210402_14_27_50_Pro',\n",
       " 'WIN_20210407_09_04_05_Pro',\n",
       " 'WIN_20210402_19_04_53_Pro',\n",
       " 'WIN_20210416_08_06_54_Pro',\n",
       " 'Video_1',\n",
       " 'WIN_20210408_14_00_44_Pro',\n",
       " 'WIN_20210404_21_41_12_Pro',\n",
       " 'WIN_20210330_13_10_29_Pro']"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "video_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataframes = []\n",
    "for i in video_names: \n",
    "    list_dataframes.append(create_dataframe_video('/Users/valentinadiproietto/OpenFace/processed/', i))\n"
   ]
  },
  {
   "source": [
    "## PRELIMINARY STUDY OF FEATURES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    226\n",
      "Name: success, dtype: int64\n",
      "0    339\n",
      "Name: success, dtype: int64\n",
      "Series([], Name: success, dtype: int64)\n",
      "0    12\n",
      "Name: success, dtype: int64\n",
      "0    118\n",
      "Name: success, dtype: int64\n",
      "0    11\n",
      "Name: success, dtype: int64\n",
      "0    340\n",
      "Name: success, dtype: int64\n",
      "0    35\n",
      "Name: success, dtype: int64\n",
      "Series([], Name: success, dtype: int64)\n",
      "0    121\n",
      "Name: success, dtype: int64\n",
      "0    506\n",
      "Name: success, dtype: int64\n",
      "0    15\n",
      "Name: success, dtype: int64\n",
      "0    1\n",
      "Name: success, dtype: int64\n",
      "0    9\n",
      "Name: success, dtype: int64\n",
      "0    264\n",
      "Name: success, dtype: int64\n",
      "Series([], Name: success, dtype: int64)\n",
      "0    22\n",
      "Name: success, dtype: int64\n",
      "Series([], Name: success, dtype: int64)\n",
      "0    208\n",
      "Name: success, dtype: int64\n",
      "0    122\n",
      "Name: success, dtype: int64\n",
      "0    5\n",
      "Name: success, dtype: int64\n",
      "Series([], Name: success, dtype: int64)\n",
      "0    70\n",
      "Name: success, dtype: int64\n",
      "0    14279\n",
      "1       24\n",
      "Name: success, dtype: int64\n",
      "Series([], Name: success, dtype: int64)\n",
      "0    12\n",
      "Name: success, dtype: int64\n",
      "Series([], Name: success, dtype: int64)\n",
      "Series([], Name: success, dtype: int64)\n",
      "Series([], Name: success, dtype: int64)\n",
      "0    307\n",
      "Name: success, dtype: int64\n",
      "0    130\n",
      "Name: success, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for df in list_dataframes:\n",
    "    print(df[df[\"confidence\"]<0.7][\"success\"].value_counts())"
   ]
  },
  {
   "source": [
    "In every dataframe if confidence is less then 0.7, success is always 0. If confidence is between 0.7 and 0.8, there are cases of success. Therefore I think that one can avoid using the confidence column, we can consider that sucess is a goo way to distinguish between cases of success or not. Be carefull, even if success is equal to zero there are many values that are not equal to zero. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "len(list_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "98.34157695664824\n93.96474986647677\n100.0\n99.9474444882407\n99.22779922779922\n99.96171516079633\n95.07823613086771\n99.87678987678987\n100.0\n99.51358405504804\n98.44551626678135\n99.9315399484536\n99.9953288490284\n99.96298428888706\n98.88920191572423\n100.0\n99.88188260948573\n100.0\n98.7196683331301\n99.5310891693035\n99.97950399672064\n100.0\n99.69153483453047\n47.2531935317138\n100.0\n99.9057048561999\n100.0\n100.0\n100.0\n99.08576533650982\n99.39536601126188\n"
     ]
    }
   ],
   "source": [
    "for i in list_dataframes:\n",
    "    print(check_success(i))"
   ]
  },
  {
   "source": [
    "The video \"'WIN_20210402_14_27_50_Pro'\" is problematic for OpenFace, in less than 50% of the frames OpenFace succeded in detecting a face, but in all the other cases OpenFace was succesful, what do we do with this video? We erase the video."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list_dataframes[0]"
   ]
  },
  {
   "source": [
    "The columns 'AUXX_c' for XX in  ['01','02', '04', '05', '06', '07', '09', '10','12', '14', '15', '17', '20', '23', '25','26', '28', '45'] contain only 0's and 1's where 1 correspond to the presence of the corresponding AUXX, while 0 represents the absence. The columns 'AUXX_r' for XX in  for XX in  ['01','02', '04', '05', '06', '07', '09', '10','12', '14', '15', '17', '20', '23', '25','26', '45'] represent the intensity from 1 to 5 of the respective AU (with continuous value in between). REMARK: OpenFace does not compute the intesity for AU2 which is the lip suck."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 35 columns corresponding to AU, 18 which mesure the intensity and 17 that measure if the AU is detected\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for i in df.columns:\n",
    "    if i.startswith(\"AU\"):\n",
    "        count+=1\n",
    "print(f\"There are {count} columns corresponding to AU, {count-count//2} which mesure the intensity and {count//2} that measure if the AU is detected\")               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9978, 714)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1        0.01\n",
       "2        0.04\n",
       "3        0.04\n",
       "4        0.03\n",
       "8        0.01\n",
       "         ... \n",
       "13709    0.04\n",
       "13710    0.04\n",
       "13721    0.05\n",
       "13722    0.05\n",
       "13723    0.05\n",
       "Name: AU01_r, Length: 3422, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "print(df.loc[df['AU01_c']==0].shape)\n",
    "\n",
    "df.loc[df['AU01_c']==0].loc[df['AU01_r']!=0]['AU01_r']"
   ]
  },
  {
   "source": [
    "The reason is explained in the github \"NOTE that the intensity and presence predictors have been trained separately and on slightly different datasets, this means that the predictions of both might not always be consistent (e.g. the presence model could be predicting AU as not being present, but the intensity model could be predicting its value above 1).\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "ABOUT gaze: to extract it uses a Constrained Local Neural Field (CLNF) landmark detector with which it determined the eyes landmarks, and then comput the gaze direction for each eye and the angle. \n",
    "\n",
    "gaze_0_x, gaze_0_y, gaze_0_z Eye gaze direction vector in world coordinates for eye 0 (normalized), eye 0 is the leftmost eye in the image (think of it as a ray going from the left eye in the image in the direction of the eye gaze)\n",
    "\n",
    "gaze_1_x, gaze_1_y, gaze_1_z Eye gaze direction vector in world coordinates for eye 1 (normalized), eye 1 is the rightmost eye in the image (think of it as a ray going from the right eye in the image in the direction of the eye gaze)\n",
    "\n",
    "gaze_angle_x, gaze_angle_y Eye gaze direction in radians in world coordinates averaged for both eyes and converted into more easy to use format than gaze vectors. If a person is looking left-right this will results in the change of gaze_angle_x (from positive to negative) and, if a person is looking up-down this will result in change of gaze_angle_y (from negative to positive), if a person is looking straight ahead both of the angles will be close to 0 (within measurement error).\n",
    "\n",
    "eye_lmk_x_0, eye_lmk_x_1,... eye_lmk_x55, eye_lmk_y_1,... eye_lmk_y_55 location of 2D eye region landmarks in pixels. The landmark index can be found below\n",
    "\n",
    "eye_lmk_X_0, eye_lmk_X_1,... eye_lmk_X55, eye_lmk_Y_0,... eye_lmk_Z_55 location of 3D eye region landmarks in millimeters. The landmark index can be found below\n",
    "\n",
    "\n",
    "THEREFORE WE CAN TAKE gaze_0_x, gaze_0_y, gaze_0_z, gaze_1_x, gaze_1_y, gaze_1_z, gaze_angle_x, gaze_angle_y\n",
    "\n",
    "OR EVEN SIMPLER gaze_angle_x, gaze_angle_y, angle averaged over the two eyes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "HEAD POSE, this is calculated from the facial landmarks extracted with a  Convolutional Experts Constrained Local Model (CE-CLM) \n",
    "\n",
    "pose_Tx, pose_Ty, pose_Tz the location of the head with respect to camera in millimeters (positive Z is away from the camera)\n",
    "\n",
    "pose_Rx, pose_Ry, pose_Rz Rotation is in radians around X,Y,Z axes with the convention R = Rx \\* Ry \\* Rz, left-handed positive sign. This can be seen as pitch (Rx), yaw (Ry), and roll (Rz). The rotation is in world coordinates with camera being the origin. x-axis=left-right, z-axis =in-out, y-axis = up-down, the Rx, Rz, Ry represent the angle of rotation with respect to a given axis.\n",
    "\n",
    "So to see how much the head moves, I think we need both."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The feature that starts with p are parameters used in the  point distribution model (PDM) which is used to detect facial landmark and the position of the face. They are parameters used by the model, so I think we can avoid to use them. The author says \"Local parameters (34 non-rigid) are parameters controlling non-rigid face shape, in other words things like facial expression and face shape (long, vs round, vs wide faces etc.). In my thesis they are q in equation 4.6 and are explained in detail in section 4.2.2 (https://www.cl.cam.ac.uk/~tb346/pub/thesis/phd_thesis.pdf)\n",
    "\n",
    "If you are familiar with Principal Component Analysis (PCA), they are the dimensionality reduced components of the 68 facial landmarks. So individually each can represent a particular basis which roughly corresponds to an expression or face shape, but are not always easily mapped to one. If you want to visualize them and see their effect, have a look at matlab_version/pdm_generation/visualise_PDMs.m, this will visualize the first 5 ps and what effect on the face they have.\n",
    "\n",
    "The global parameters, on the other hand, control the placement of the face in the image and are responsible for translation (x and y), scale/size, and the rotation of the face.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['frame', 'face_id', 'timestamp', 'confidence', 'success', 'gaze_0_x',\n",
       "       'gaze_0_y', 'gaze_0_z', 'gaze_1_x', 'gaze_1_y', 'gaze_1_z',\n",
       "       'gaze_angle_x', 'gaze_angle_y', 'eye_lmk_x_0', 'eye_lmk_x_1',\n",
       "       'eye_lmk_x_2', 'eye_lmk_x_3', 'eye_lmk_x_4', 'eye_lmk_x_5',\n",
       "       'eye_lmk_x_6', 'eye_lmk_x_7', 'eye_lmk_x_8', 'eye_lmk_x_9',\n",
       "       'eye_lmk_x_10', 'eye_lmk_x_11', 'eye_lmk_x_12', 'eye_lmk_x_13',\n",
       "       'eye_lmk_x_14', 'eye_lmk_x_15', 'eye_lmk_x_16', 'eye_lmk_x_17',\n",
       "       'eye_lmk_x_18', 'eye_lmk_x_19', 'eye_lmk_x_20', 'eye_lmk_x_21',\n",
       "       'eye_lmk_x_22', 'eye_lmk_x_23', 'eye_lmk_x_24', 'eye_lmk_x_25',\n",
       "       'eye_lmk_x_26', 'eye_lmk_x_27', 'eye_lmk_x_28', 'eye_lmk_x_29',\n",
       "       'eye_lmk_x_30', 'eye_lmk_x_31', 'eye_lmk_x_32', 'eye_lmk_x_33',\n",
       "       'eye_lmk_x_34', 'eye_lmk_x_35', 'eye_lmk_x_36', 'eye_lmk_x_37',\n",
       "       'eye_lmk_x_38', 'eye_lmk_x_39', 'eye_lmk_x_40', 'eye_lmk_x_41',\n",
       "       'eye_lmk_x_42', 'eye_lmk_x_43', 'eye_lmk_x_44', 'eye_lmk_x_45',\n",
       "       'eye_lmk_x_46', 'eye_lmk_x_47', 'eye_lmk_x_48', 'eye_lmk_x_49',\n",
       "       'eye_lmk_x_50', 'eye_lmk_x_51', 'eye_lmk_x_52', 'eye_lmk_x_53',\n",
       "       'eye_lmk_x_54', 'eye_lmk_x_55', 'eye_lmk_y_0', 'eye_lmk_y_1',\n",
       "       'eye_lmk_y_2', 'eye_lmk_y_3', 'eye_lmk_y_4', 'eye_lmk_y_5',\n",
       "       'eye_lmk_y_6', 'eye_lmk_y_7', 'eye_lmk_y_8', 'eye_lmk_y_9',\n",
       "       'eye_lmk_y_10', 'eye_lmk_y_11', 'eye_lmk_y_12', 'eye_lmk_y_13',\n",
       "       'eye_lmk_y_14', 'eye_lmk_y_15', 'eye_lmk_y_16', 'eye_lmk_y_17',\n",
       "       'eye_lmk_y_18', 'eye_lmk_y_19', 'eye_lmk_y_20', 'eye_lmk_y_21',\n",
       "       'eye_lmk_y_22', 'eye_lmk_y_23', 'eye_lmk_y_24', 'eye_lmk_y_25',\n",
       "       'eye_lmk_y_26', 'eye_lmk_y_27', 'eye_lmk_y_28', 'eye_lmk_y_29',\n",
       "       'eye_lmk_y_30', 'eye_lmk_y_31', 'eye_lmk_y_32', 'eye_lmk_y_33',\n",
       "       'eye_lmk_y_34', 'eye_lmk_y_35', 'eye_lmk_y_36', 'eye_lmk_y_37',\n",
       "       'eye_lmk_y_38', 'eye_lmk_y_39', 'eye_lmk_y_40', 'eye_lmk_y_41',\n",
       "       'eye_lmk_y_42', 'eye_lmk_y_43', 'eye_lmk_y_44', 'eye_lmk_y_45',\n",
       "       'eye_lmk_y_46', 'eye_lmk_y_47', 'eye_lmk_y_48', 'eye_lmk_y_49',\n",
       "       'eye_lmk_y_50', 'eye_lmk_y_51', 'eye_lmk_y_52', 'eye_lmk_y_53',\n",
       "       'eye_lmk_y_54', 'eye_lmk_y_55', 'eye_lmk_X_0', 'eye_lmk_X_1',\n",
       "       'eye_lmk_X_2', 'eye_lmk_X_3', 'eye_lmk_X_4', 'eye_lmk_X_5',\n",
       "       'eye_lmk_X_6', 'eye_lmk_X_7', 'eye_lmk_X_8', 'eye_lmk_X_9',\n",
       "       'eye_lmk_X_10', 'eye_lmk_X_11', 'eye_lmk_X_12', 'eye_lmk_X_13',\n",
       "       'eye_lmk_X_14', 'eye_lmk_X_15', 'eye_lmk_X_16', 'eye_lmk_X_17',\n",
       "       'eye_lmk_X_18', 'eye_lmk_X_19', 'eye_lmk_X_20', 'eye_lmk_X_21',\n",
       "       'eye_lmk_X_22', 'eye_lmk_X_23', 'eye_lmk_X_24', 'eye_lmk_X_25',\n",
       "       'eye_lmk_X_26', 'eye_lmk_X_27', 'eye_lmk_X_28', 'eye_lmk_X_29',\n",
       "       'eye_lmk_X_30', 'eye_lmk_X_31', 'eye_lmk_X_32', 'eye_lmk_X_33',\n",
       "       'eye_lmk_X_34', 'eye_lmk_X_35', 'eye_lmk_X_36', 'eye_lmk_X_37',\n",
       "       'eye_lmk_X_38', 'eye_lmk_X_39', 'eye_lmk_X_40', 'eye_lmk_X_41',\n",
       "       'eye_lmk_X_42', 'eye_lmk_X_43', 'eye_lmk_X_44', 'eye_lmk_X_45',\n",
       "       'eye_lmk_X_46', 'eye_lmk_X_47', 'eye_lmk_X_48', 'eye_lmk_X_49',\n",
       "       'eye_lmk_X_50', 'eye_lmk_X_51', 'eye_lmk_X_52', 'eye_lmk_X_53',\n",
       "       'eye_lmk_X_54', 'eye_lmk_X_55', 'eye_lmk_Y_0', 'eye_lmk_Y_1',\n",
       "       'eye_lmk_Y_2', 'eye_lmk_Y_3', 'eye_lmk_Y_4', 'eye_lmk_Y_5',\n",
       "       'eye_lmk_Y_6', 'eye_lmk_Y_7', 'eye_lmk_Y_8', 'eye_lmk_Y_9',\n",
       "       'eye_lmk_Y_10', 'eye_lmk_Y_11', 'eye_lmk_Y_12', 'eye_lmk_Y_13',\n",
       "       'eye_lmk_Y_14', 'eye_lmk_Y_15', 'eye_lmk_Y_16', 'eye_lmk_Y_17',\n",
       "       'eye_lmk_Y_18', 'eye_lmk_Y_19', 'eye_lmk_Y_20', 'eye_lmk_Y_21',\n",
       "       'eye_lmk_Y_22', 'eye_lmk_Y_23', 'eye_lmk_Y_24', 'eye_lmk_Y_25',\n",
       "       'eye_lmk_Y_26', 'eye_lmk_Y_27', 'eye_lmk_Y_28', 'eye_lmk_Y_29',\n",
       "       'eye_lmk_Y_30', 'eye_lmk_Y_31', 'eye_lmk_Y_32', 'eye_lmk_Y_33',\n",
       "       'eye_lmk_Y_34', 'eye_lmk_Y_35', 'eye_lmk_Y_36', 'eye_lmk_Y_37',\n",
       "       'eye_lmk_Y_38', 'eye_lmk_Y_39', 'eye_lmk_Y_40', 'eye_lmk_Y_41',\n",
       "       'eye_lmk_Y_42', 'eye_lmk_Y_43', 'eye_lmk_Y_44', 'eye_lmk_Y_45',\n",
       "       'eye_lmk_Y_46', 'eye_lmk_Y_47', 'eye_lmk_Y_48', 'eye_lmk_Y_49',\n",
       "       'eye_lmk_Y_50', 'eye_lmk_Y_51', 'eye_lmk_Y_52', 'eye_lmk_Y_53',\n",
       "       'eye_lmk_Y_54', 'eye_lmk_Y_55', 'eye_lmk_Z_0', 'eye_lmk_Z_1',\n",
       "       'eye_lmk_Z_2', 'eye_lmk_Z_3', 'eye_lmk_Z_4', 'eye_lmk_Z_5',\n",
       "       'eye_lmk_Z_6', 'eye_lmk_Z_7', 'eye_lmk_Z_8', 'eye_lmk_Z_9',\n",
       "       'eye_lmk_Z_10', 'eye_lmk_Z_11', 'eye_lmk_Z_12', 'eye_lmk_Z_13',\n",
       "       'eye_lmk_Z_14', 'eye_lmk_Z_15', 'eye_lmk_Z_16', 'eye_lmk_Z_17',\n",
       "       'eye_lmk_Z_18', 'eye_lmk_Z_19', 'eye_lmk_Z_20', 'eye_lmk_Z_21',\n",
       "       'eye_lmk_Z_22', 'eye_lmk_Z_23', 'eye_lmk_Z_24', 'eye_lmk_Z_25',\n",
       "       'eye_lmk_Z_26', 'eye_lmk_Z_27', 'eye_lmk_Z_28', 'eye_lmk_Z_29',\n",
       "       'eye_lmk_Z_30', 'eye_lmk_Z_31', 'eye_lmk_Z_32', 'eye_lmk_Z_33',\n",
       "       'eye_lmk_Z_34', 'eye_lmk_Z_35', 'eye_lmk_Z_36', 'eye_lmk_Z_37',\n",
       "       'eye_lmk_Z_38', 'eye_lmk_Z_39', 'eye_lmk_Z_40', 'eye_lmk_Z_41',\n",
       "       'eye_lmk_Z_42', 'eye_lmk_Z_43', 'eye_lmk_Z_44', 'eye_lmk_Z_45',\n",
       "       'eye_lmk_Z_46', 'eye_lmk_Z_47', 'eye_lmk_Z_48', 'eye_lmk_Z_49',\n",
       "       'eye_lmk_Z_50', 'eye_lmk_Z_51', 'eye_lmk_Z_52', 'eye_lmk_Z_53',\n",
       "       'eye_lmk_Z_54', 'eye_lmk_Z_55', 'pose_Tx', 'pose_Ty', 'pose_Tz',\n",
       "       'pose_Rx', 'pose_Ry', 'pose_Rz', 'x_0', 'x_1', 'x_2', 'x_3', 'x_4',\n",
       "       'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_11', 'x_12', 'x_13',\n",
       "       'x_14', 'x_15', 'x_16', 'x_17', 'x_18', 'x_19', 'x_20', 'x_21', 'x_22',\n",
       "       'x_23', 'x_24', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31',\n",
       "       'x_32', 'x_33', 'x_34', 'x_35', 'x_36', 'x_37', 'x_38', 'x_39', 'x_40',\n",
       "       'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49',\n",
       "       'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58',\n",
       "       'x_59', 'x_60', 'x_61', 'x_62', 'x_63', 'x_64', 'x_65', 'x_66', 'x_67',\n",
       "       'y_0', 'y_1', 'y_2', 'y_3', 'y_4', 'y_5', 'y_6', 'y_7', 'y_8', 'y_9',\n",
       "       'y_10', 'y_11', 'y_12', 'y_13', 'y_14', 'y_15', 'y_16', 'y_17', 'y_18',\n",
       "       'y_19', 'y_20', 'y_21', 'y_22', 'y_23', 'y_24', 'y_25', 'y_26', 'y_27',\n",
       "       'y_28', 'y_29', 'y_30', 'y_31', 'y_32', 'y_33', 'y_34', 'y_35', 'y_36',\n",
       "       'y_37', 'y_38', 'y_39', 'y_40', 'y_41', 'y_42', 'y_43', 'y_44', 'y_45',\n",
       "       'y_46', 'y_47', 'y_48', 'y_49', 'y_50', 'y_51', 'y_52', 'y_53', 'y_54',\n",
       "       'y_55', 'y_56', 'y_57', 'y_58', 'y_59', 'y_60', 'y_61', 'y_62', 'y_63',\n",
       "       'y_64', 'y_65', 'y_66', 'y_67', 'X_0', 'X_1', 'X_2', 'X_3', 'X_4',\n",
       "       'X_5', 'X_6', 'X_7', 'X_8', 'X_9', 'X_10', 'X_11', 'X_12', 'X_13',\n",
       "       'X_14', 'X_15', 'X_16', 'X_17', 'X_18', 'X_19', 'X_20', 'X_21', 'X_22',\n",
       "       'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28', 'X_29', 'X_30', 'X_31',\n",
       "       'X_32', 'X_33', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_39', 'X_40',\n",
       "       'X_41', 'X_42', 'X_43', 'X_44', 'X_45', 'X_46', 'X_47', 'X_48', 'X_49',\n",
       "       'X_50', 'X_51', 'X_52', 'X_53', 'X_54', 'X_55', 'X_56', 'X_57', 'X_58',\n",
       "       'X_59', 'X_60', 'X_61', 'X_62', 'X_63', 'X_64', 'X_65', 'X_66', 'X_67',\n",
       "       'Y_0', 'Y_1', 'Y_2', 'Y_3', 'Y_4', 'Y_5', 'Y_6', 'Y_7', 'Y_8', 'Y_9',\n",
       "       'Y_10', 'Y_11', 'Y_12', 'Y_13', 'Y_14', 'Y_15', 'Y_16', 'Y_17', 'Y_18',\n",
       "       'Y_19', 'Y_20', 'Y_21', 'Y_22', 'Y_23', 'Y_24', 'Y_25', 'Y_26', 'Y_27',\n",
       "       'Y_28', 'Y_29', 'Y_30', 'Y_31', 'Y_32', 'Y_33', 'Y_34', 'Y_35', 'Y_36',\n",
       "       'Y_37', 'Y_38', 'Y_39', 'Y_40', 'Y_41', 'Y_42', 'Y_43', 'Y_44', 'Y_45',\n",
       "       'Y_46', 'Y_47', 'Y_48', 'Y_49', 'Y_50', 'Y_51', 'Y_52', 'Y_53', 'Y_54',\n",
       "       'Y_55', 'Y_56', 'Y_57', 'Y_58', 'Y_59', 'Y_60', 'Y_61', 'Y_62', 'Y_63',\n",
       "       'Y_64', 'Y_65', 'Y_66', 'Y_67', 'Z_0', 'Z_1', 'Z_2', 'Z_3', 'Z_4',\n",
       "       'Z_5', 'Z_6', 'Z_7', 'Z_8', 'Z_9', 'Z_10', 'Z_11', 'Z_12', 'Z_13',\n",
       "       'Z_14', 'Z_15', 'Z_16', 'Z_17', 'Z_18', 'Z_19', 'Z_20', 'Z_21', 'Z_22',\n",
       "       'Z_23', 'Z_24', 'Z_25', 'Z_26', 'Z_27', 'Z_28', 'Z_29', 'Z_30', 'Z_31',\n",
       "       'Z_32', 'Z_33', 'Z_34', 'Z_35', 'Z_36', 'Z_37', 'Z_38', 'Z_39', 'Z_40',\n",
       "       'Z_41', 'Z_42', 'Z_43', 'Z_44', 'Z_45', 'Z_46', 'Z_47', 'Z_48', 'Z_49',\n",
       "       'Z_50', 'Z_51', 'Z_52', 'Z_53', 'Z_54', 'Z_55', 'Z_56', 'Z_57', 'Z_58',\n",
       "       'Z_59', 'Z_60', 'Z_61', 'Z_62', 'Z_63', 'Z_64', 'Z_65', 'Z_66', 'Z_67',\n",
       "       'p_scale', 'p_rx', 'p_ry', 'p_rz', 'p_tx', 'p_ty', 'p_0', 'p_1', 'p_2',\n",
       "       'p_3', 'p_4', 'p_5', 'p_6', 'p_7', 'p_8', 'p_9', 'p_10', 'p_11', 'p_12',\n",
       "       'p_13', 'p_14', 'p_15', 'p_16', 'p_17', 'p_18', 'p_19', 'p_20', 'p_21',\n",
       "       'p_22', 'p_23', 'p_24', 'p_25', 'p_26', 'p_27', 'p_28', 'p_29', 'p_30',\n",
       "       'p_31', 'p_32', 'p_33', 'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r',\n",
       "       'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r',\n",
       "       'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r', 'AU01_c',\n",
       "       'AU02_c', 'AU04_c', 'AU05_c', 'AU06_c', 'AU07_c', 'AU09_c', 'AU10_c',\n",
       "       'AU12_c', 'AU14_c', 'AU15_c', 'AU17_c', 'AU20_c', 'AU23_c', 'AU25_c',\n",
       "       'AU26_c', 'AU28_c', 'AU45_c'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "pd.options.display.max_seq_items = 2000\n",
    "df.columns"
   ]
  },
  {
   "source": [
    "Therefor we have decide to keep only the following features: features_to_keep = [\n",
    "    'frame','face_id','timestamp','confidence','success',\n",
    "\n",
    "    'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r',\n",
    "    'AU01_c', 'AU02_c', 'AU04_c', 'AU05_c', 'AU06_c', 'AU07_c', 'AU09_c', 'AU10_c', 'AU12_c', 'AU14_c', 'AU15_c', 'AU17_c', 'AU20_c', 'AU23_c', 'AU25_c', 'AU26_c', 'AU28_c', 'AU45_c',\n",
    "\n",
    "    'gaze_0_x','gaze_0_y', 'gaze_0_z', 'gaze_1_x', 'gaze_1_y', 'gaze_1_z',\n",
    "    'gaze_angle_x', 'gaze_angle_y', \n",
    "    'pose_Tx', 'pose_Ty', 'pose_Tz','pose_Rx', 'pose_Ry', 'pose_Rz',\n",
    "\n",
    "    'type_candidat','sexe','video_name','stress_global','stress','diapo'\n",
    "]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ELIMINATE PROBLEMATIC VIDEOS FROM THE LIST "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names.remove('WIN_20210329_14_13_45_Pro')\n",
    "video_names.remove('WIN_20210402_14_27_50_Pro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "len(video_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 29/29 [03:44<00:00,  7.75s/it]\n"
     ]
    }
   ],
   "source": [
    "list_df_max = []\n",
    "for v_name in tqdm(video_names):\n",
    "    df_annoted = get_df_video_with_annotations('/Users/valentinadiproietto/OpenFace/processed/', v_name, filename_annotations, \"max\")\n",
    "    list_df_max.append(eliminate_features(df_annoted))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "source": [
    "## 3 JUNE 2021, ADDING FIRST AND SECOND DERIVATIVES\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_deriv = []\n",
    "for i in list_df_max:\n",
    "    to_drop = ['frame','face_id','timestamp','confidence','success', 'type_candidat']\n",
    "    i = add_frameTimeWindow(i)\n",
    "    i = i.drop(to_drop, axis = 1)\n",
    "    i = add_derivatives_drop_spatial(i)\n",
    "    df_with_deriv.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_features= ['video_name','stress_global','stress','frameTimeWindow','sexe']\n",
    "\n",
    "df_total = pd.concat(df_with_deriv)\n",
    "df_total = df_total.drop('diapo', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "stress\n",
       "1.0       2377\n",
       "0.0       2015\n",
       "2.0        633\n",
       "3.0         20\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "\n",
    "df_total = df_total.groupby(groupby_features).agg(['mean']).reset_index()\n",
    "df_total.columns= df_total.columns.map('_'.join).str.strip('_')\n",
    "df_total[['stress']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['sexe', 'AU01_r_mean', 'AU02_r_mean', 'AU04_r_mean', 'AU05_r_mean',\n",
       "       'AU06_r_mean', 'AU07_r_mean', 'AU09_r_mean', 'AU10_r_mean',\n",
       "       'AU12_r_mean', 'AU14_r_mean', 'AU15_r_mean', 'AU17_r_mean',\n",
       "       'AU20_r_mean', 'AU23_r_mean', 'AU25_r_mean', 'AU26_r_mean',\n",
       "       'AU45_r_mean', 'AU01_c_mean', 'AU02_c_mean', 'AU04_c_mean',\n",
       "       'AU05_c_mean', 'AU06_c_mean', 'AU07_c_mean', 'AU09_c_mean',\n",
       "       'AU10_c_mean', 'AU12_c_mean', 'AU14_c_mean', 'AU15_c_mean',\n",
       "       'AU17_c_mean', 'AU20_c_mean', 'AU23_c_mean', 'AU25_c_mean',\n",
       "       'AU26_c_mean', 'AU28_c_mean', 'AU45_c_mean', 'AU01_r_de_mean',\n",
       "       'AU01_r_de_de_mean', 'AU02_r_de_mean', 'AU02_r_de_de_mean',\n",
       "       'AU04_r_de_mean', 'AU04_r_de_de_mean', 'AU05_r_de_mean',\n",
       "       'AU05_r_de_de_mean', 'AU06_r_de_mean', 'AU06_r_de_de_mean',\n",
       "       'AU07_r_de_mean', 'AU07_r_de_de_mean', 'AU09_r_de_mean',\n",
       "       'AU09_r_de_de_mean', 'AU10_r_de_mean', 'AU10_r_de_de_mean',\n",
       "       'AU12_r_de_mean', 'AU12_r_de_de_mean', 'AU14_r_de_mean',\n",
       "       'AU14_r_de_de_mean', 'AU15_r_de_mean', 'AU15_r_de_de_mean',\n",
       "       'AU17_r_de_mean', 'AU17_r_de_de_mean', 'AU20_r_de_mean',\n",
       "       'AU20_r_de_de_mean', 'AU23_r_de_mean', 'AU23_r_de_de_mean',\n",
       "       'AU25_r_de_mean', 'AU25_r_de_de_mean', 'AU26_r_de_mean',\n",
       "       'AU26_r_de_de_mean', 'AU45_r_de_mean', 'AU45_r_de_de_mean',\n",
       "       'gaze_0_x_de_mean', 'gaze_0_x_de_de_mean', 'gaze_0_y_de_mean',\n",
       "       'gaze_0_y_de_de_mean', 'gaze_0_z_de_mean', 'gaze_0_z_de_de_mean',\n",
       "       'gaze_1_x_de_mean', 'gaze_1_x_de_de_mean', 'gaze_1_y_de_mean',\n",
       "       'gaze_1_y_de_de_mean', 'gaze_1_z_de_mean', 'gaze_1_z_de_de_mean',\n",
       "       'gaze_angle_x_de_mean', 'gaze_angle_x_de_de_mean',\n",
       "       'gaze_angle_y_de_mean', 'gaze_angle_y_de_de_mean', 'pose_Tx_de_mean',\n",
       "       'pose_Tx_de_de_mean', 'pose_Ty_de_mean', 'pose_Ty_de_de_mean',\n",
       "       'pose_Tz_de_mean', 'pose_Tz_de_de_mean', 'pose_Rx_de_mean',\n",
       "       'pose_Rx_de_de_mean', 'pose_Ry_de_mean', 'pose_Ry_de_de_mean',\n",
       "       'pose_Rz_de_mean', 'pose_Rz_de_de_mean'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "x = df_total.drop(['video_name','stress_global',\t'stress','frameTimeWindow'], axis = 1)\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace string values\n",
    "x.sexe = x.sexe.replace('H',0)\n",
    "x.sexe = x.sexe.replace('F',1)\n",
    "\n",
    "y = df_total[['stress']]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "y_train = y_train.values.squeeze()\n",
    "y_test = y_test.values.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score =  0.7766900508619785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "print(\"F1 score = \", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT TRAIN AND TEST W RT VIDEOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names_train = video_names[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names_test =  video_names[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_drop = df_total.drop(['stress_global','frameTimeWindow'], axis = 1)\n",
    "df_total_drop.sexe = df_total_drop.sexe.replace('H',0)\n",
    "df_total_drop.sexe = df_total_drop.sexe.replace('F',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_train = df_total_drop[df_total_drop[\"video_name\"].isin(video_names_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test =df_total_drop[df_total_drop[\"video_name\"].isin(video_names_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.drop(['stress', 'video_name'], axis =1)\n",
    "y_train = df_train[['stress']]\n",
    "x_test = df_test.drop(['stress', 'video_name'], axis =1)\n",
    "y_test = df_test[['stress']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train = y_train.values.squeeze()\n",
    "y_test = y_test.values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score =  0.37453722297806136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "print(\"F1 score = \", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "source": [
    "## 8 JUIN 2021, CV WITH LEAVE ONE VIDEO OUT "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 29/29 [02:23<00:00,  4.96s/it]\n"
     ]
    }
   ],
   "source": [
    "list_df_max = []\n",
    "for v_name in tqdm(video_names):\n",
    "    df_annoted = get_df_video_with_annotations('/Users/valentinadiproietto/OpenFace/processed/', v_name, filename_annotations, \"max\")\n",
    "    list_df_max.append(eliminate_features(df_annoted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_deriv = []\n",
    "for i in list_df_max:\n",
    "    to_drop = ['frame','face_id','timestamp','confidence','success', 'type_candidat']\n",
    "    i = add_frameTimeWindow(i)\n",
    "    i = i.drop(to_drop, axis = 1)\n",
    "    i = add_derivatives_drop_spatial(i)\n",
    "    df_with_deriv.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_features= ['video_name','stress_global','stress','frameTimeWindow','sexe']\n",
    "\n",
    "df_total = pd.concat(df_with_deriv)\n",
    "df_total = df_total.drop('diapo', axis = 1)\n",
    "\n",
    "df_total = df_total.groupby(groupby_features).agg(['mean']).reset_index()\n",
    "df_total.columns= df_total.columns.map('_'.join).str.strip('_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.concat([df_total[['stress']], df_total[['video_name']]], axis = 1 )\n",
    "x = df_total.drop(['stress_global',\t'stress','frameTimeWindow'], axis = 1)\n",
    "x.sexe = x.sexe.replace('H',0)\n",
    "x.sexe = x.sexe.replace('F',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "groups = df_total['video_name']\n",
    "loo = LeaveOneGroupOut()\n",
    "\n",
    "cv_loo = loo.split(x, y, groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_no_name = x.drop(['video_name'], axis = 1)\n",
    "y_no_name = y.drop(['video_name'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "model = RandomForestClassifier(random_state = 42)\n",
    "y_pred= cross_val_predict(model, x_no_name, y_no_name, cv=cv_loo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score =  0.4247932417279817\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score = \", f1_score(y_no_name, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fare per diapo et per timewindow e metterlo nel drive parametri. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd0dd1cf4766d5d9df855e04619ca954236457012a0dce4d3d89890d807f4ecf8a5",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}